import load_radicals as rad
import load_sentences as sen
import string

sub_dataset = []
for pair in sen.dataset:
    substitute = []
    for c in pair[1]:
        substitute.extend(rad.kanji_dict[c] if (c in rad.kanji_dict) else [c])
    sub_dataset.append((pair[0], substitute))

hiragana_katakana = [
    'ー',
    '〜',
    'ぁ',
    'あ',
    'ぃ',
    'い',
    'ぅ',
    'う',
    'ぇ',
    'え',
    'ぉ',
    'お',
    'か',
    'が',
    'き',
    'ぎ',
    'く',
    'ぐ',
    'け',
    'げ',
    'こ',
    'ご',
    'さ',
    'ざ',
    'し',
    'じ',
    'す',
    'ず',
    'せ',
    'ぜ',
    'そ',
    'ぞ',
    'た',
    'だ',
    'ち',
    'ぢ',
    'っ',
    'つ',
    'て',
    'で',
    'と',
    'ど',
    'な',
    'に',
    'ぬ',
    'ね',
    'の',
    'は',
    'ば',
    'ぱ',
    'ひ',
    'び',
    'ぴ',
    'ふ',
    'ぶ',
    'ぷ',
    'へ',
    'べ',
    'ぺ',
    'ほ',
    'ぼ',
    'ぽ',
    'ま',
    'み',
    'む',
    'め',
    'も',
    'ゃ',
    'や',
    'ゅ',
    'ゆ',
    'ょ',
    'よ',
    'ら',
    'り',
    'る',
    'れ',
    'ろ',
    'ゎ',
    'わ',
    'う',
    'ぃ',
    'を',
    'ん',
    'ァ',
    'ア',
    'ィ',
    'イ',
    'ゥ',
    'ウ',
    'ェ',
    'エ',
    'ォ',
    'オ',
    'カ',
    'ガ',
    'キ',
    'ギ',
    'ク',
    'グ',
    'ケ',
    'ゲ',
    'コ',
    'ゴ',
    'サ',
    'ザ',
    'シ',
    'ジ',
    'ス',
    'ズ',
    'セ',
    'ゼ',
    'ソ',
    'ゾ',
    'タ',
    'ダ',
    'チ',
    'ヂ',
    'ッ',
    'ツ',
    'ヅ',
    'テ',
    'デ',
    'ト',
    'ド',
    'ナ',
    'ニ',
    'ヌ',
    'ネ',
    'ノ',
    'ハ',
    'バ',
    'オ',
    'ア',
    'ヒ',
    'ビ',
    'ピ',
    'フ',
    'ブ',
    'プ',
    'ヘ',
    'ベ',
    'ペ',
    'ホ',
    'ボ',
    'ポ',
    'マ',
    'ミ',
    'ム',
    'メ',
    'モ',
    'ャ',
    'ヤ',
    'ュ',
    'ユ',
    'ョ',
    'ヨ',
    'ラ',
    'リ',
    'ル',
    'レ',
    'ロ',
    'ヮ',
    'ワ',
    'ヲ',
    'ン',
    'ヴ',
    'ヵ',
    'ヶ',
    '”',
    '。',
    '！',
    '？',
    '、',
    '・',
    '０',
    '１',
    '２',
    '３',
    '４',
    '５',
    '６',
    '７',
    '８',
    '９'
]

en_charset = [c for c in string.printable] + ['padding']
jp_charset = rad.radicals_list + hiragana_katakana + [c for c in string.printable] + ['padding']

token_dataset = []
for en, jp in sub_dataset:
    en_tokenized = []
    for c in en:
        try:
            en_tokenized.append(en_charset.index(c) + 1)
        except ValueError:
            print("%s not found in en charset, adding" % c)
            en_charset.append(c)
            en_tokenized.append(en_charset.index(c))
    jp_tokenized = []
    for c in jp:
        try:
            jp_tokenized.append(jp_charset.index(c) + 1)
        except ValueError:
            print("%s not found in jp charset, adding" % c)
            jp_charset.append(c)
            jp_tokenized.append(jp_charset.index(c))
    token_dataset.append((en_tokenized, jp_tokenized))
