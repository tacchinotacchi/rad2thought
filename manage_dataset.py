import load_radicals as rad
import load_sentences as sen
import numpy as np
import string

hiragana_katakana = [
    'ー',
    '〜',
    '～',
    'ぁ',
    'あ',
    'ぃ',
    'い',
    'ぅ',
    'う',
    'ぇ',
    'え',
    'ぉ',
    'お',
    'か',
    'が',
    'き',
    'ぎ',
    'く',
    'ぐ',
    'け',
    'げ',
    'こ',
    'ご',
    'さ',
    'ざ',
    'し',
    'じ',
    'す',
    'ず',
    'せ',
    'ぜ',
    'そ',
    'ぞ',
    'た',
    'だ',
    'ち',
    'ぢ',
    'っ',
    'つ',
    'づ',
    'て',
    'で',
    'と',
    'ど',
    'な',
    'に',
    'ぬ',
    'ね',
    'の',
    'は',
    'ば',
    'ぱ',
    'ひ',
    'び',
    'ぴ',
    'ふ',
    'ぶ',
    'ぷ',
    'へ',
    'べ',
    'ぺ',
    'ほ',
    'ぼ',
    'ぽ',
    'ま',
    'み',
    'む',
    'め',
    'も',
    'ゃ',
    'や',
    'ゅ',
    'ゆ',
    'ょ',
    'よ',
    'ら',
    'り',
    'る',
    'れ',
    'ろ',
    'ゎ',
    'わ',
    'う',
    'ぃ',
    'を',
    'ん',
    'ァ',
    'ア',
    'ィ',
    'イ',
    'ゥ',
    'ウ',
    'ェ',
    'エ',
    'ォ',
    'オ',
    'カ',
    'ガ',
    'キ',
    'ギ',
    'ク',
    'グ',
    'ケ',
    'ゲ',
    'コ',
    'ゴ',
    'サ',
    'ザ',
    'シ',
    'ジ',
    'ス',
    'ズ',
    'セ',
    'ゼ',
    'ソ',
    'ゾ',
    'タ',
    'ダ',
    'チ',
    'ヂ',
    'ッ',
    'ツ',
    'ヅ',
    'テ',
    'デ',
    'ト',
    'ド',
    'ナ',
    'ニ',
    'ヌ',
    'ネ',
    'ノ',
    'ハ',
    'バ',
    'パ',
    'オ',
    'ア',
    'ヒ',
    'ビ',
    'ピ',
    'フ',
    'ブ',
    'プ',
    'ヘ',
    'ベ',
    'ペ',
    'ホ',
    'ボ',
    'ポ',
    'マ',
    'ミ',
    'ム',
    'メ',
    'モ',
    'ャ',
    'ヤ',
    'ュ',
    'ユ',
    'ョ',
    'ヨ',
    'ラ',
    'リ',
    'ル',
    'レ',
    'ロ',
    'ヮ',
    'ワ',
    'ヲ',
    'ン',
    'ヴ',
    'ヵ',
    'ヶ',
    '”',
    '。',
    '！',
    '？',
    '、',
    '・',
    '－',
    '（',
    '）',
    '「',
    '」',
    '＝',
    '：',
    '．',
    '　',
    '—',
    '＋',
    '，',
    '％',
    '々',
    'ａ',
    'ｂ',
    'ｃ',
    'ｄ',
    'ｅ',
    'ｆ',
    'ｇ',
    'ｈ',
    'ｉ',
    'ｊ',
    'ｋ',
    'ｌ',
    'ｍ',
    'ｎ',
    'ｏ',
    'ｐ',
    'ｑ',
    'ｒ',
    'ｔ',
    'ｓ',
    'ｕ',
    'ｖ',
    'ｗ',
    'ｘ',
    'ｙ',
    'ｚ',
    'Ａ',
    'Ｂ',
    'Ｃ',
    'Ｄ',
    'Ｅ',
    'Ｆ',
    'Ｇ',
    'Ｈ',
    'Ｉ',
    'Ｊ',
    'Ｋ',
    'Ｌ',
    'Ｍ',
    'Ｎ',
    'Ｏ',
    'Ｐ',
    'Ｑ',
    'Ｒ',
    'Ｔ',
    'Ｓ',
    'Ｕ',
    'Ｖ',
    'Ｗ',
    'Ｘ',
    'Ｙ',
    'Ｚ',
    '０',
    '１',
    '２',
    '３',
    '４',
    '５',
    '６',
    '７',
    '８',
    '９'
]

base_charset = [c for c in string.printable] + [
    '€',
    'ñ',
    'é',
    '’',
    '℃',
    '―'
]

en_charset = ['<padding>'] + ['<unk>'] + base_charset
jp_charset = ['<padding>'] + ['<unk>'] + base_charset + rad.radicals_list + hiragana_katakana

def expand_kanji(original):
    substitute = []
    for c in original:
        substitute.extend(rad.kanji_dict[c] if (c in rad.kanji_dict) else [c])
    return substitute

def tokenize_sentence(original, charset):
    tokenized = np.empty(len(original) + 2, dtype=np.int64)
    tokenized[0] = len(charset)
    for index, c in enumerate(original):
        tokenized[index + 1] = charset.index(c)
    tokenized[-1] = len(charset) + 1
    return tokenized

print("Tokenizing...")
token_dataset = []
error_sentences = 0
index = 0
for en, jp in sen.dataset_raw:
    try:
        en_tokenized = tokenize_sentence(en, en_charset)
        jp = expand_kanji(jp)
        jp_tokenized = tokenize_sentence(jp, jp_charset)
        token_dataset.append((en_tokenized, jp_tokenized, en, jp))
    except ValueError as e:
        error_sentences += 1
        #print("char not found in charset: " + str(e))
    if index % 100000 == 0:
        print("sentence %d processed, %d errors" % (index, error_sentences))
    index += 1

def decode(sentence, charset):
    return ''.join(map(lambda t: charset[t], sentence))

def decode_en(sentence):
    return decode(sentence, en_charset)

def decode_jp(sentence):
    return decode(sentence, jp_charset)
